{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb298bb7",
   "metadata": {},
   "source": [
    "# Mini-challenge 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c909dd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 4063 messages\n",
      "3872 mbdata\n",
      "\tdropping 1645 spam\n",
      "191 ccdata\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "# read\n",
    "subset_1 = pd.read_csv('csv-1700-1830.csv',encoding='ansi')\n",
    "subset_2 = pd.read_csv('csv-1831-2000.csv',encoding='ansi')\n",
    "subset_3 = pd.read_csv('csv-2001-2131.csv',encoding='ansi')\n",
    "dset = pd.concat([subset_1, subset_2, subset_3], axis=0)\n",
    "dset = dset.astype({'date(yyyyMMddHHmmss)': 'string'})\n",
    "print(f'loaded {len(dset)} messages')\n",
    "\n",
    "\n",
    "# process date&time\n",
    "hrs = []\n",
    "mins = []\n",
    "secs = []\n",
    "for e in dset['date(yyyyMMddHHmmss)']:\n",
    "    hrs.append(int(e[8:10]))\n",
    "    mins.append(int(e[10:12]))\n",
    "    secs.append(int(e[12:14]))\n",
    "dset.insert(2, \"Second\", secs, True)\n",
    "dset.insert(2, \"Minute\", mins, True)\n",
    "dset.insert(2, \"Hour\", hrs, True)\n",
    "dset = dset.drop('date(yyyyMMddHHmmss)',axis=1)\n",
    "\n",
    "# split and remove unneeded columns\n",
    "mbdata = dset[dset['type']=='mbdata']\n",
    "print(f'{len(mbdata)} mbdata')\n",
    "mbdata = mbdata.drop('type',axis=1)\n",
    "mbdata = mbdata.drop(' location',axis=1)\n",
    "blacklist = ['KronosQuoth', 'Clevvah4Evah']\n",
    "url_check = re.compile(r\".*\\..*/.*\")\n",
    "to_drop = []\n",
    "for i,e in mbdata.iterrows():\n",
    "    if e['author'] in blacklist:\n",
    "        to_drop.append(i)\n",
    "    else:\n",
    "        result = url_check.match(str(e['message']))\n",
    "        if result:\n",
    "            to_drop.append(i)\n",
    "print(f'\\tdropping {len(to_drop)} spam')\n",
    "mbdata.drop(to_drop, axis=0, inplace=True)\n",
    "mbdata = mbdata.reset_index(drop=True)\n",
    "ccdata = dset[dset['type']=='ccdata']\n",
    "print(f'{len(ccdata)} ccdata')\n",
    "ccdata = ccdata.drop('type',axis=1)\n",
    "ccdata = ccdata.drop('latitude',axis=1)\n",
    "ccdata = ccdata.drop('longitude',axis=1)\n",
    "ccdata = ccdata.drop('author',axis=1)\n",
    "ccdata.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5c63cd",
   "metadata": {},
   "source": [
    "# Differentiating junk from humans\n",
    "The easiest type of junk to filter are messages with links. For this dataset, any post with a URL in it seems to be spam. Slightly more difficult to filter are the accounts like KronosQuoth, which seems to post regular inspirational and thought-provoking quotes. I chose a simple approach of scanning over the dataset and blacklisting accounts that were clearly doing this. To expand this approach, I could disregard any posts that retweet these blacklisted accounts. A more advanced method of detecting these junk accounts might be to look over each user individually and look for telltale signs. Accounts like this seem to use the same hashtags in every tweet, as well as proper punctuation. This lack of variety could be detected with an algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6309eb3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mike\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mike\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8164/377983138.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'author'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcasefold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mforbidden_words\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mforbidden_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'author'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcasefold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mstop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"english\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0mfiltered\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import FreqDist\n",
    "from nltk.draw import dispersion\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# tokenize text and filter out stop & forbidden words\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "word_chunk = []\n",
    "forbidden_words = ['rt', 'e']\n",
    "\n",
    "# we don't want to track who gets retweeted the most, so add all usernames to the forbidden list\n",
    "for i,e in mbdata.iterrows():\n",
    "    word_chunk.extend(tokenizer.tokenize(e['message']))\n",
    "    if e['author'].casefold() not in forbidden_words:\n",
    "        forbidden_words.append(e['author'].casefold())\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered = []\n",
    "for w in word_chunk:\n",
    "    if w.casefold() not in stop_words and w.casefold() not in forbidden_words:\n",
    "        filtered.append(w.casefold())\n",
    "\n",
    "# get a frequency distribution\n",
    "fq = FreqDist(filtered)\n",
    "fq.plot(25, cumulative=False)\n",
    "\n",
    "# plots for the top 25\n",
    "dwords = {}\n",
    "for w in fq.most_common(15):\n",
    "    dwords[w[0]] = []\n",
    "for i,e in mbdata.iterrows():\n",
    "    for k in dwords:\n",
    "        rexpr = re.compile(r'.*' + k + '.*')\n",
    "        match = rexpr.match(str(e['message']).casefold())\n",
    "        if match:\n",
    "            time = 3600*e['Hour'] + 60*e['Minute'] + e['Second']\n",
    "            dwords[k].append(time)\n",
    "for k in dwords:\n",
    "    plt.plot(dwords[k], range(len(dwords[k])))\n",
    "plt.legend(dwords.keys())\n",
    "plt.show()\n",
    "\n",
    "# plots for the selected words\n",
    "plt.figure()\n",
    "dwords = {'fire': [],\n",
    "          'rally': [],\n",
    "          'police': [],\n",
    "          'van': [],\n",
    "          'hostage': [],\n",
    "          'hostages': [],\n",
    "          'standoff': [],\n",
    "          'shooting': []\n",
    "         }\n",
    "dispersion.dispersion_plot(word_chunk, list(dwords.keys()))\n",
    "for i,e in mbdata.iterrows():\n",
    "    for k in dwords:\n",
    "        rexpr = re.compile(r'.*' + k + '.*')\n",
    "        match = rexpr.match(str(e['message']).casefold())\n",
    "        if match:\n",
    "            time = 3600*e['Hour'] + 60*e['Minute'] + e['Second']\n",
    "            dwords[k].append(time)\n",
    "for k in dwords:\n",
    "    plt.plot(dwords[k], range(len(dwords[k])))\n",
    "plt.legend(dwords.keys())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2dfb5d",
   "metadata": {},
   "source": [
    "# Identifying Level of Risk\n",
    "One method of approaching this is to monitor the frequency of certain keywords associated with risk. Plots are presented for two sets of keywords. The first set of plots is for the 25 words most commonly used in the dataset. There are many seemingly mundane words in the set, but they all show a sharp rise about halfway through the time period. This suggests that emergencies create a rise in social media activity as people post about the emergency at hand. The most commonly used word in the time period was 'Abila', the site of the fire. The second set of plots represents a few words that would be indicative of danger. At the time of each accident, related words experienced an increase in usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "044fe251",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8164/2893549444.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0maddress_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m's'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'way'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'street'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'st'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rd'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'way'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ave'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'nan'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'na'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcasefold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcasefold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maddress_words\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mfiltered\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcasefold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stop_words' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# tokenize text\n",
    "word_chunk = []\n",
    "for i,e in ccdata.iterrows():\n",
    "    word_chunk.extend(tokenizer.tokenize(str(e[' location'])))\n",
    "filtered = []\n",
    "address_words = ['n', 's', 'way', 'street', 'st', 'rd', 'way', 'ave', 'nan', 'na']\n",
    "for w in word_chunk:\n",
    "    if w.casefold() not in stop_words and w.casefold() not in address_words:\n",
    "        filtered.append(w.casefold())\n",
    "\n",
    "# find the most common street names among police responses\n",
    "fq = FreqDist(filtered)\n",
    "fq.plot(10, cumulative=False)\n",
    "\n",
    "# find the time they occur\n",
    "dwords = {}\n",
    "for w in fq.most_common(10):\n",
    "    dwords[w[0]] = []\n",
    "for i,e in ccdata.iterrows():\n",
    "    for k in dwords:\n",
    "        rexpr = re.compile(r'.*' + k + '.*')\n",
    "        match = rexpr.match(str(e[' location']).casefold())\n",
    "        if match:\n",
    "            time = 3600*e['Hour'] + 60*e['Minute'] + e['Second']\n",
    "            dwords[k].append(time)\n",
    "for k in dwords:\n",
    "    plt.plot(dwords[k], range(len(dwords[k])))\n",
    "plt.legend(dwords.keys())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64164d52",
   "metadata": {},
   "source": [
    "# First Responders\n",
    "By tracking the frequency of street names in the 'location' field of police reports, we can see when each location experienced a spike in report frequency. Multiple reports in a short time period are indicative of emergency situations. However, this method fails to take the type of report in regard. For example, traffic stops and drunk subjects would not be prioritized by first responders in the event of a fire or act of terrorism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d097c3",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "NLP was a reasonably effective method of processing this data. With more time and ability, I think that more precise conclusions could have been drawn with extensions of the methods I used. However, despite my own limitations I could analyze the dataset and come to solid conclusions based on my analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216c9838",
   "metadata": {},
   "source": [
    "# Report Questions\n",
    "1. Using visual analytics, characterize the different types of content in the dataset. What distinguishes meaningful event reports from typical chatter from junk or spam?  Please limit your answer to 8 images and 500 words.\n",
    "\n",
    "As mentioned above, spam posts always include either a link or a repeated quote with the same hashtags in every post.\n",
    "\n",
    "2. Use visual analytics to represent and evaluate how the level of the risk to the public evolves over the course of the evening. Consider the potential consequences of the situation and the number of people who could be affected.  Please limit your answer to 10 images and 1000 words.\n",
    "\n",
    "See above for visual analytics. Risk is greatest during the two spikes in police reports, during the fire and hostage crisis.\n",
    "\n",
    "3. If you were able to send a team of first responders to any single place, where would it be? Provide your rationale. How might your response be different if you had to respond to the events in real time rather than retrospectively? Please limit your answer to 8 images and 500 words.\n",
    "\n",
    "I would send a team to Achilleous St. The fire created the largest amount of police reports, and is a high-risk event that should be taken care of swiftly. The hostage crisis is a far more significant and severe event, yes, but requires more of a response than first responders, and a good deal of delicacy.\n",
    "\n",
    "4. If you solved this mini-challenge in 2014, how did you approach it differently this year?\n",
    "n/a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3089911",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
