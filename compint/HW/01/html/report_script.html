
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Homework 1</title><meta name="generator" content="MATLAB 9.10"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2021-09-01"><meta name="DC.source" content="report_script.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; }

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }
span.typesection { color:#A0522D }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>Homework 1</h1><!--introduction--><p>For this assignment, a Perceptron Learning Algorithm was implemented in MATLAB.</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Part 0: Creating the Data Set</a></li><li><a href="#2">Part 1: PLA and Testing</a></li><li><a href="#3"><b>N=10</b></a></li><li><a href="#4"><b>N=100</b></a></li><li><a href="#5">Part 2: Modifying PLA</a></li><li><a href="#6"><b>$\eta$=.001</b></a></li><li><a href="#7"><b>$\eta$=1</b></a></li><li><a href="#8"><b>$\eta$=100</b></a></li></ul></div><h2 id="1">Part 0: Creating the Data Set</h2><p>To create a data set, the provided <tt>pla_getdata.m</tt> is used, with an additional output <tt>f</tt> added, which returns the hidden target function:</p><pre class="codeinput">N = 10; <span class="comment">% data size</span>
[x, y, f] = pla_getdata(-1, 1, 2, 11*N);
</pre><h2 id="2">Part 1: PLA and Testing</h2><p>The PLA algorithm was implemented in <tt>pla.m</tt>, and a testing script <tt>pla_test.m</tt> was created for simple use. The weight vector <tt>w</tt> is preset as [0; 1; 1.5] and then adjusted by the PLA algorithm. Testing was done with two data sizes - N=10 and N=100 - and the average accuracy and iterations were found over 1000 runs</p><h2 id="3"><b>N=10</b></h2><p>The results of this size varied wildly. With just 10 points in the training set, there's a higher likelihood that the training set does not accurately reflect the target function. In such cases, accuracy could be extremely low. The average accuracy across 1000 runs was calculated to be 89.5%, and the average number of iterations per hypothesis was 11.2. From this data, the approximate disagreement can be calculated to be 10.5%. Plots compare an example hypothesis <tt>g</tt> to the hidden function <tt>f</tt> with a training set.</p><pre class="codeinput">[x, y, g, f, i, a] = pla_test(10, 1000);
pla_plot_f(x, y, g, f);
mi = mean(i)
ma = mean(a)
</pre><h2 id="4"><b>N=100</b></h2><p>Using this size resulted in a much more precise PLA, with an accuracy of 98.7% over 1000 runs, with an approximate disagreement of 1.3%. The cost of this is processing speed. Hypotheses took an average of 111.7 iterations to be judged as accurate enough.</p><pre class="codeinput">[x, y, g, f, i, a] = pla_test(100, 1000);
pla_plot_f(x, y, g, f);
mi = mean(i)
ma = mean(a)
</pre><pre class="codeoutput">
mi =

   10.6850


ma =

    0.8894


mi =

  104.3120


ma =

    0.9871

</pre><h2 id="5">Part 2: Modifying PLA</h2><p>To further increase the PLA's accuracy, distance can be factored in when adjusting the weights of each hypothesis. Two more variables are thus added: distance <tt>s(t)</tt> and the learning factor <img src="report_script_eq17615028512695387165.png" alt="$\eta$" style="width:7px;height:10px;">. For this, two new functions were created. The algorithm is trained with a set of 100 points and tested on a set of 10,000, and its iterations are limited to 1000 per guess. <img src="report_script_eq17615028512695387165.png" alt="$\eta$" style="width:7px;height:10px;"> is tested for values of .0001, .01, 1, and 100. Again, plots compare an example hypothesis <tt>g</tt> to the hidden function <tt>f</tt> with a training set.</p><h2 id="6"><b>$\eta$=.001</b></h2><pre class="codeinput">[x, y, g, f, i, a] = pla_test_s(100, 1000, .0001);
pla_plot_f(x, y, g, f);
mi = mean(i)
ma = mean(a)

<span class="comment">% *$\eta$=.1*</span>
[x, y, g, f, i, a] = pla_test_s(100, 1000, .01);
pla_plot_f(x, y, g, f);
mi = mean(i)
ma = mean(a)
</pre><h2 id="7"><b>$\eta$=1</b></h2><pre class="codeinput">[x, y, g, f, i, a] = pla_test_s(100, 1000, 1);
pla_plot_f(x, y, g, f);
mi = mean(i)
ma = mean(a)
</pre><h2 id="8"><b>$\eta$=100</b></h2><pre class="codeinput">[x, y, g, f, i, a] = pla_test_s(100, 1000, 100);
pla_plot_f(x, y, g, f);
mi = mean(i)
ma = mean(a)

<span class="comment">% Limiting the iterations for this step is a necessary move. Especially for</span>
<span class="comment">% the case of $\eta$=.0001, the hypothesizing process just happens too</span>
<span class="comment">% slowly with so slow a learning rate.</span>
</pre><p class="footer"><br><a href="https://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2021a</a><br></p></div><!--
##### SOURCE BEGIN #####
%% Homework 1
% For this assignment, a Perceptron Learning Algorithm was implemented in 
% MATLAB.

%% Part 0: Creating the Data Set
% To create a data set, the provided |pla_getdata.m| is used, with an 
% additional output |f| added, which returns the hidden target function:
N = 10; % data size
[x, y, f] = pla_getdata(-1, 1, 2, 11*N);

%% Part 1: PLA and Testing
% The PLA algorithm was implemented in |pla.m|, and a testing script
% |pla_test.m| was created for simple use. The weight vector |w| is preset
% as [0; 1; 1.5] and then adjusted by the PLA algorithm. Testing was done
% with two data sizes - N=10 and N=100 - and the average accuracy and
% iterations were found over 1000 runs

%%% *N=10*
% The results of this size varied wildly. With just 10 points in the
% training set, there's a higher likelihood that the training set does not
% accurately reflect the target function. In such cases, accuracy could be
% extremely low. The average accuracy across 1000 runs was calculated to
% be 89.5%, and the average number of iterations per hypothesis was 11.2.
% From this data, the approximate disagreement can be calculated to be
% 10.5%. Plots compare an example hypothesis |g| to the hidden function |f|
% with a training set.
[x, y, g, f, i, a] = pla_test(10, 1000);
pla_plot_f(x, y, g, f);
mi = mean(i)
ma = mean(a)

%%% *N=100*
% Using this size resulted in a much more precise PLA, with an accuracy of
% 98.7% over 1000 runs, with an approximate disagreement of 1.3%. The cost 
% of this is processing speed. Hypotheses took an average of 111.7 
% iterations to be judged as accurate enough.
[x, y, g, f, i, a] = pla_test(100, 1000);
pla_plot_f(x, y, g, f);
mi = mean(i)
ma = mean(a)

%% Part 2: Modifying PLA
% To further increase the PLA's accuracy, distance can be factored in when
% adjusting the weights of each hypothesis. Two more variables are thus 
% added: distance |s(t)| and the learning factor $\eta$. For this, two new
% functions were created. The algorithm is trained with a set of 100 points
% and tested on a set of 10,000, and its iterations are limited to 1000 per
% guess. $\eta$ is tested for values of .0001, .01, 1, and 100. Again, plots 
% compare an example hypothesis |g| to the hidden function |f| with a
% training set.

%%% *$\eta$=.001*
[x, y, g, f, i, a] = pla_test_s(100, 1000, .0001);
pla_plot_f(x, y, g, f);
mi = mean(i)
ma = mean(a)

% *$\eta$=.1*
[x, y, g, f, i, a] = pla_test_s(100, 1000, .01);
pla_plot_f(x, y, g, f);
mi = mean(i)
ma = mean(a)

%%% *$\eta$=1*
[x, y, g, f, i, a] = pla_test_s(100, 1000, 1);
pla_plot_f(x, y, g, f);
mi = mean(i)
ma = mean(a)

%%% *$\eta$=100*
[x, y, g, f, i, a] = pla_test_s(100, 1000, 100);
pla_plot_f(x, y, g, f);
mi = mean(i)
ma = mean(a)

% Limiting the iterations for this step is a necessary move. Especially for
% the case of $\eta$=.0001, the hypothesizing process just happens too
% slowly with so slow a learning rate.




##### SOURCE END #####
--></body></html>